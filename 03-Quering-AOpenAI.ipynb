{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59d527f-1100-45ff-b051-5f7c9029d94d",
   "metadata": {},
   "source": [
    "# Queries with and without Azure OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f6c7e3-9037-4b1e-ae17-1deaa27b9c08",
   "metadata": {},
   "source": [
    "## Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e50b404-a061-49e7-a3c7-c6eabc98ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import requests\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display, HTML\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from app.embeddings import OpenAIEmbeddings\n",
    "from app.prompts import STUFF_PROMPT, REFINE_PROMPT, REFINE_QUESTION_PROMPT\n",
    "from app.credentials import (\n",
    "    DATASOURCE_CONNECTION_STRING,\n",
    "    AZURE_SEARCH_API_VERSION,\n",
    "    AZURE_SEARCH_ENDPOINT,\n",
    "    AZURE_SEARCH_KEY,\n",
    "    COG_SERVICES_NAME,\n",
    "    COG_SERVICES_KEY,\n",
    "    AZURE_OPENAI_ENDPOINT,\n",
    "    AZURE_OPENAI_KEY,\n",
    "    AZURE_OPENAI_API_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f2c22f8-79ab-405c-95e8-77a1978e53bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Payloads header\n",
    "headers = {'Content-Type': 'application/json','api-key': AZURE_SEARCH_KEY}\n",
    "params = {'api-version': AZURE_SEARCH_API_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297d29b-1f61-4dce-858e-bf4272172dba",
   "metadata": {},
   "source": [
    "## Multi-Index Search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a46e2d3-298a-4708-83de-9e108b1a117a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Index that we are going to query (from Notebook 01 and 02)\n",
    "index1_name = \"cogsrch-snowflake-index-files\"\n",
    "indexes = [index1_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b53c14-19bd-451f-aa43-7ad27ccfeead",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"What is Snowflake?\" \n",
    "# This questions is interesting since CLP means something in Computer science and means something different in medical field "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d925eb-7f9c-429e-a62a-4c37d7702caf",
   "metadata": {},
   "source": [
    "### Search on both indexes individually and aggragate results\n",
    "\n",
    "Note: In order to standarize the indexes we are setting 4 mandatory fields to be present on each index: id, title, content, pages, language. These fields must be present in each index so that each document can be treated the same along the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf2e30f-e71f-4533-ab52-27d048b80a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://azure-cog-search-cstevuxaqrxcm.search.windows.net/indexes/cogsrch-snowflake-index-files/docs?api-version=2021-04-30-Preview&search=What is Snowflake?&select=*&$top=10&queryLanguage=en-us&queryType=semantic&semanticConfiguration=my-semantic-config&$count=true&speller=lexicon&answers=extractive|count-3&captions=extractive|highlight-false\n",
      "200\n",
      "Results Found: 3, Results Returned: 3\n"
     ]
    }
   ],
   "source": [
    "agg_search_results = []\n",
    "\n",
    "for index in indexes:\n",
    "    url = AZURE_SEARCH_ENDPOINT + '/indexes/'+ index + '/docs'\n",
    "    url += '?api-version={}'.format(AZURE_SEARCH_API_VERSION)\n",
    "    url += '&search={}'.format(QUESTION)\n",
    "    url += '&select=*'\n",
    "    url += '&$top=10'  # You can change this to anything you need/want\n",
    "    url += '&queryLanguage=en-us'\n",
    "    url += '&queryType=semantic'\n",
    "    url += '&semanticConfiguration=my-semantic-config'\n",
    "    url += '&$count=true'\n",
    "    url += '&speller=lexicon'\n",
    "    url += '&answers=extractive|count-3'\n",
    "    url += '&captions=extractive|highlight-false'\n",
    "\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    print(url)\n",
    "    print(resp.status_code)\n",
    "\n",
    "    search_results = resp.json()\n",
    "    agg_search_results.append(search_results)\n",
    "    print(\"Results Found: {}, Results Returned: {}\".format(search_results['@odata.count'], len(search_results['value'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd0fe5-4ee0-42e2-a920-72b93a407389",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Display the top results (from both searches) based on the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e938337-602d-4b61-8141-b8c92a5d91da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Top Answers</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>Answer - score: 0.99560546875</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Snowflake is a   massively parallel processing (MPP) database that   is fully relational, ACID compliant, and processes   standard SQL natively without translation or   simulation. It was designed as a software service that   can take full advantage of cloud infrastructure, while   retaining the positive attributes of existing solutions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>Answer - score: 0.90380859375</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Snowflake provides out-of-the-box network access control via “network policies”, allowing users to restrict   account access to specific IP addresses. The level of granularity can be account-level and user-specific   (bear in mind that user settings take precedence when you assign policies for both)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h4>Top Results</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>19336_Preview&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 2.0341796875</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Snowflake provides out-of-the-box network access control via “network policies”, allowing users to restrict   account access to specific IP addresses. The level of granularity can be account-level and user-specific   (bear in mind that user settings take precedence when you assign policies for both)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>Untitled&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 1.86328125</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Snowflake is a   massively parallel processing (MPP) database that   is fully relational, ACID compliant, and processes   standard SQL natively without translation or   simulation. It was designed as a software service that   can take full advantage of cloud infrastructure, while   retaining the positive attributes of existing solutions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>None&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: 1.4697265625</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "1  the snowflake elastic data warehouse sigmod 2016 and beyond  ashish motivala, jiaqi yan        2  our product  •the snowflake elastic data warehouse, or “snowflake” •built for the cloud •multi-tenant, transactional, secure, highly scalable, elastic •implemented from scratch (no hadoop, postgres etc.)  •currently runs on aws and azure  •serves …"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML('<h4>Top Answers</h4>'))\n",
    "\n",
    "for search_results in agg_search_results:\n",
    "    for result in search_results['@search.answers']:\n",
    "        if result['score'] > 0.5: # Show answers that are at least 50% of the max possible score=1\n",
    "            display(HTML('<h5>' + 'Answer - score: ' + str(result['score']) + '</h5>'))\n",
    "            display(HTML(result['text']))\n",
    "            \n",
    "print(\"\\n\\n\")\n",
    "display(HTML('<h4>Top Results</h4>'))\n",
    "\n",
    "file_content = OrderedDict()\n",
    "\n",
    "for search_results in agg_search_results:\n",
    "    for result in search_results['value']:\n",
    "        if result['@search.rerankerScore'] > 0.4: # Show results that are at least 10% of the max possible score=4\n",
    "            display(HTML('<h5>' + str(result['title']) + '&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score: '+ str(result['@search.rerankerScore']) + '</h5>'))\n",
    "            display(HTML(result['@search.captions'][0]['text']))\n",
    "            file_content[result['id']]={\n",
    "                                    \"title\": result['title'],\n",
    "                                    \"chunks\": result['pages'],\n",
    "                                    \"language\": result['language'], \n",
    "                                    \"caption\": result['@search.captions'][0]['text'],\n",
    "                                    \"score\": result['@search.rerankerScore'],\n",
    "                                    \"location\": result['metadata_storage_path']                  \n",
    "                                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3e6d4-9a09-4b0f-b328-238738ccfaec",
   "metadata": {},
   "source": [
    "# Using Azure OpenAI\n",
    "\n",
    "Of course we want OpenAI to give a better answer, so we instead of sending these results, we send the content of the documents of the search result articles to OpenAI and lets GPT model give the answer.\n",
    "\n",
    "The problem is that the content of the search result files is or can be very lengthy, more than the allowed tokens allowed by the GPT Azure OpenAI models. So what we need to do is to split in chunks, vectorize and do a vector semantic search. \n",
    "\n",
    "Notice that **the documents chunks are already done in Azure Search**. file_content dictionary (created in the cell above) contains the pages (chunks) of each document. So we dont really need to chunk them again, each doc page for sure will fit on the max tokens limit of the completions LLM and of the embedding LLM.\n",
    "\n",
    "We will use a genius library call LangChain that wraps a lot of boiler plate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eea62a7d-7e0e-4a93-a89c-20c96560c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_KEY\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"] = AZURE_OPENAI_API_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f7b41d2-65b0-4058-8a46-c76cf6960720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 30\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for key,value in file_content.items():\n",
    "    for page in value[\"chunks\"]:\n",
    "        docs.append(Document(page_content=page, metadata={\"source\": value[\"location\"]}))\n",
    "        \n",
    "print(\"Number of chunks:\",len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5403dee-a4c4-420c-9819-68151d973695",
   "metadata": {},
   "source": [
    "Depending of the amount of chunks/pages returned from the search result, which is very related to the size of the documents returned\n",
    "we pick the embedding model that give us fast results. <br>The logic is, if there is less than 50 chunks (of 5000 chars each) to vectorize then we use \n",
    "OpenAI models which currently don't offer batch processing, but if there is more than 50 chunks we use a BERT based in-memory model that processes in batches and in parallel (it is recommended a VM of at least 4 cores).\n",
    "\n",
    "For more information on in-memory models that you can use, see [HERE](https://www.sbert.net/docs/pretrained_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a03f1f10-32b0-4c1e-8a0e-eee1b1d29ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the Embedder model\n",
    "if len(docs) < 50:\n",
    "    # OpenAI models are accurate but slower\n",
    "    embedder = OpenAIEmbeddings(document_model_name=\"text-embedding-ada-002\", query_model_name=\"text-embedding-ada-002\") \n",
    "else:\n",
    "    # Bert based models are faster (3x-10x) but not as great in accuracy as OpenAI models\n",
    "    # Since this repo supports Multiple languages we need to use a multilingual model. \n",
    "    # But if English only is the requirement, use \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "    # The fastest english model is \"all-MiniLM-L12-v2\"\n",
    "    if random.choice(list(file_content.items()))[1][\"language\"] == \"en\":\n",
    "        embedder = HuggingFaceEmbeddings(model_name = 'multi-qa-MiniLM-L6-cos-v1')\n",
    "    else:\n",
    "        embedder = HuggingFaceEmbeddings(model_name = 'distiluse-base-multilingual-cased-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5cc7ae5-6fe9-4fd8-992d-0507297387ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, document_model_name='text-embedding-ada-002', query_model_name='text-embedding-ada-002', openai_api_key=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3315033a-4a08-4db5-8f5c-fa0a99892dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 139 ms, sys: 28.1 ms, total: 167 ms\n",
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if(len(docs)>1):\n",
    "    db = FAISS.from_documents(docs, embedder)\n",
    "else:\n",
    "    print(\"No results Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57429335-34d3-458a-b7c9-52482a0936d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_db = db.similarity_search(QUESTION, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17247488-7d14-4178-9add-31eb1afcbcbe",
   "metadata": {},
   "source": [
    "At this point we already have the most similar chunks (in order of relevance given by the in-memory vector cosine similarity search) in docs_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793c4788-715e-44dd-b2b8-3f1c3201e4e0",
   "metadata": {},
   "source": [
    "### Now we use GPT-3.5(Turbo) using map-reduce chain in order to stay within the limits of the allow model's token count\n",
    "\n",
    "for more information on the different types of prompts for these chains please see here:\n",
    "\n",
    "https://github.com/hwchase17/langchain/tree/master/langchain/chains/question_answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "634f5bd8-0d56-47b8-84fd-fe9b678bfc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the deployment named \"gpt-35-turbo\" for the model \"gpt-35-turbo (0301)\". \n",
    "# Use \"gpt-4\" if you have it available.\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-35-turbo\", temperature=0.9, max_tokens=500)\n",
    "chain = load_qa_with_sources_chain(llm, chain_type=\"map_reduce\", return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1e619b8-1dcf-431b-8aad-f1696a09c2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2082 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 839 ms, sys: 100 ms, total: 939 ms\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = chain({\"input_documents\": docs_db, \"question\": QUESTION}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cddb1cb-a4a0-4e2f-9f0c-4216b0f232b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Azure OpenAI ChatGPT Answer:</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake is an enterprise-ready data warehouse as a service that is cloud-based, highly scalable, elastic, and supports columnar storage and processing of semi-structured data. It offers automatic type inference and columnar storage for schema-less data, encrypted data import and export, encryption of table data, role-based access control, and two-factor authentication. \n",
      "Sources:\n",
      "[' https://depprocureformstorage.blob.core.windows.net/snowflake-data/Snowflake%20Internals.pdf and https://depprocureformstorage.blob.core.windows.net/snowflake-data/Snowflake_basic%20overview.pdf']\n"
     ]
    }
   ],
   "source": [
    "answer = response['output_text']\n",
    "\n",
    "display(HTML('<h4>Azure OpenAI ChatGPT Answer:</h4>'))\n",
    "\n",
    "print(answer.split(\"Source:\")[0])\n",
    "print(\"Sources:\")\n",
    "print(answer.split(\"Source:\")[1].replace(' ',' ').split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11345374-6420-4b36-b061-795d2a804c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n•Snowflake is an enterprise-ready data warehouse as a service\\n•Novel multi-cluster, shared-data architecture\\n•Highly elastic and available\\n•Semi-structured and schema-less data at the speed of relational data\\n•Pure SaaS experience',\n",
       " '•The Snowflake Elastic Data Warehouse, or “Snowflake”\\n•Multi-tenant, transactional, secure, highly scalable, elastic\\n•Implemented from scratch (no Hadoop, Postgres etc.)\\n•Currently runs on AWS and Azure \\n•Serves tens of millions of queries per day over hundreds petabytes of data\\n•1000+ active customers, growing fast',\n",
       " 'Snowflake is a cloud-based enterprise data warehousing service that supports self-describing, compact binary serialization designed for fast key-value lookup, comparison, and hashing. It offers columnar storage and processing of semi-structured data, automatic type inference and columnar storage for schema-less data (VARIANT), encrypted data import and export, encryption of table data using NIST 800-57 compliant hierarchical key management and key lifecycle, role-based access control (RBAC) within SQL, and two-factor authentication and federated authentication. Snowflake is an ELT platform that offers the speed and expressiveness of an RDBMS, and it supports time travel and cloning, data sharing, serverless ingestion of data, and reclustering of data. It also offers a spark connector with pushdown and support for Azure Cloud and many other connectors. The company is based in San Mateo and Seattle and was founded in August 2012.',\n",
       " 'Snowflake is a cloud data warehousing company that has reinvented the data warehouse for the cloud and today’s data. It is built from the cloud up with a patent-pending new architecture that delivers the power of data warehousing, the flexibility of big data platforms, and the elasticity of the cloud at a fraction of the cost of traditional solutions.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment if you want to inspect the results from each top similar chunk (k=4 by default)\n",
    "response['intermediate_steps']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347373a-a5be-473d-b64e-0f6b6dbcd0e0",
   "metadata": {},
   "source": [
    "# Summary\n",
    "##### This answer is way better than taking just the result from Azure Cognitive Search. So the summary is:\n",
    "- Azure Cognitive Search give us the top results (context)\n",
    "- Azure OpenAI takes these results and understand the content and uses it as context to give the best answer\n",
    "- Best of two worlds!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
